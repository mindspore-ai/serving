model_path:
    prefill_model: [
                    "/path/llama2-70b-mindir/dis/rank_0/full_graph.mindir",
                    "/path/llama2-70b-mindir/dis/rank_1/full_graph.mindir",
                    "/path/llama2-70b-mindir/dis/rank_2/full_graph.mindir",
                    "/path/llama2-70b-mindir/dis/rank_3/full_graph.mindir",
                    ]
    decode_model: [
                   "/path/llama2-70b-mindir/dis/rank_0/inc_graph.mindir",
                   "/path/llama2-70b-mindir/dis/rank_1/inc_graph.mindir",
                   "/path/llama2-70b-mindir/dis/rank_2/inc_graph.mindir",
                   "/path/llama2-70b-mindir/dis/rank_3/inc_graph.mindir",
                   ]
    argmax_model: "/path/post_process/argmax.mindir"
    topk_model: "/path/post_process/topk.mindir"
    prefill_ini : ['/path/config_ini/llma2_70b_pa_dyn_prefill.ini']
    decode_ini: [/path/config_ini/llma2_70b_pa_dyn_decode.ini']
    post_model_ini: '/path/config_static/post_config.cfg'

model_config:
    model_name: 'llama_dyn'
    max_generate_length: 4096
    end_token: 2
    seq_length: [32, 64, 512, 1024, 4096]
    vocab_size: 32000
    prefill_batch_size: [1]
    decode_batch_size: [16]
    zactivate_len: [4096]
    model_type: 'dyn'
    seq_type: 'static'
    batch_waiting_time: 0.0
    decode_batch_waiting_time: 0.0
    batching_strategy: 'continuous'
    current_index: False
    page_attention: True
    model_dtype: "DataType.FLOAT32"
    pad_token_id: 0

serving_config:
    agent_ports: [15100, 15101, 15102, 15103]
    start_device_id: 0
    server_ip: 'localhost'
    server_port: 18281

pa_config:
    num_blocks: 1024
    block_size: 128
    decode_seq_length: 4096

tokenizer:
    type: LlamaTokenizer
    vocab_file: '/path/llama/tokenizer/tokenizer.model'

basic_inputs:
    type: LlamaBasicInputs

extra_inputs:
    type: LlamaExtraInputs

warmup_inputs:
    type: LlamaWarmupInputs